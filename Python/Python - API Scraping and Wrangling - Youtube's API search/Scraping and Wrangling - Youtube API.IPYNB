{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5878b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import base64\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import tz\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pandas import json_normalize\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5216b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoutubeScraper:\n",
    "    \"\"\"\n",
    "    Thin wrapper around YouTube Data API (v3) for search and video stats.\n",
    "\n",
    "    This class provides:\n",
    "      1) `search(...)`: Save paginated search results to JSON files.\n",
    "      2) `batch(...)`: Split a DataFrame of video IDs into batches of <=50 IDs.\n",
    "      3) `video(...)`: Fetch video statistics for IDs in batches and save to disk.\n",
    "\n",
    "    Attributes:\n",
    "        BASE_URL (str): Root URL for Google APIs.\n",
    "        api_key (str): YouTube Data API key.\n",
    "        header (dict): Default HTTP headers for requests.\n",
    "    \n",
    "    How to run:\n",
    "        >>> yt = YoutubeScraper(api_key=\"YOUR_API_KEY\")\n",
    "        >>> searchAPI = yt.search(query=\"YOUR-QUERY\", type_=\"video\", max_result=50)\n",
    "        >>> videoAPI = yt.video(dataframe = \"YOUR_DATA_FRAME_NAME\")\n",
    "        \"\"\"\n",
    "    BASE_URL=\"https://www.googleapis.com\"\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "\n",
    "\n",
    "    def search(self,\n",
    "               query: str,\n",
    "               type_: str = 'video',\n",
    "               max_result: int= 50,\n",
    "               ):\n",
    "        \"\"\"Search YouTube and save each page of results as a JSON file.\n",
    "\n",
    "        Uses the `/youtube/v3/search` endpoint, follows `nextPageToken` until\n",
    "        all pages are collected, and writes each page to `data/search-<query>-<timestamp>/`.\n",
    "\n",
    "        Args:\n",
    "            query: Search keywords.\n",
    "            type_: One of {'video', 'playlist', 'channel'}.\n",
    "            max_result: Items per page (YouTube caps at 50). Must be in [1, 50].\n",
    "\n",
    "        Returns:\n",
    "            Path to the created folder containing all JSON page files.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If `max_result` or `type_` have wrong types.\n",
    "            ValueError: If `max_result` is out of range, or `type_` is invalid.\n",
    "            requests.RequestException: If the HTTP request fails (non-200/403).\n",
    "        \"\"\"\n",
    "        \n",
    "        if not isinstance(max_result,int):\n",
    "            raise TypeError('Input number 1 - 50 only')\n",
    "        if max_result > 50 or max_result < 1:\n",
    "            raise ValueError('Input number 1 - 50 only')\n",
    "    \n",
    "        if not isinstance(type_,str):\n",
    "            raise TypeError('Input video|playlist|channel')\n",
    "        if type_ != 'video' and type_ !='playlist' and type_ !='channel':\n",
    "            raise ValueError(\"Input video|playlist|channel\")\n",
    "        \n",
    "        ENDPOINT_AUTH = \"/youtube/v3/search\"\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        folder_name = os.path.join(\"data\", f\"search-{query}-{timestamp}\")\n",
    "        os.makedirs(folder_name, exist_ok=True)\n",
    "        response = {}\n",
    "\n",
    "        while True:\n",
    "            current_key = self.api_key\n",
    "            params = {'part':'snippet',\n",
    "                      'q':query,\n",
    "                      'type':type_,\n",
    "                      \"maxResults\": max_result,\n",
    "                      'key' : current_key\n",
    "                      }\n",
    "            nextpage = response.get(\"nextPageToken\", None)\n",
    "            print(f\"Getting page {nextpage}\")\n",
    "\n",
    "            if nextpage is None:\n",
    "                r = requests.get(\n",
    "                    f\"{self.BASE_URL}{ENDPOINT_AUTH}\",\n",
    "                    params=params,\n",
    "                    headers=self.header,\n",
    "                )\n",
    "                filename = f'{query}.json'\n",
    "            else:\n",
    "                params[\"pageToken\"] = nextpage\n",
    "                r = requests.get(\n",
    "                    f\"{self.BASE_URL}{ENDPOINT_AUTH}\",\n",
    "                    params=params,\n",
    "                    headers=self.header,\n",
    "                )\n",
    "                filename=f'{query}-{nextpage}.json'\n",
    "            if r.status_code == 200:\n",
    "                print(f\"Code 200 - Using Quota\")\n",
    "                response = r.json()\n",
    "                nextpage = response.get(\"nextPageToken\", None)\n",
    "                file_path = os.path.join(folder_name, filename)\n",
    "\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    json.dump(response, file, ensure_ascii=False, sort_keys=True, indent=2)\n",
    "\n",
    "                print(f\"Saved Json file page {filename}\")\n",
    "                if nextpage is None:\n",
    "                    print(\"Collected All Search\")\n",
    "                    break\n",
    "\n",
    "            elif r.status_code == 403:\n",
    "                print(\n",
    "                    f\"Key reached limit quota\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                print(f\"code {r.status_code}\")\n",
    "                break\n",
    "        return folder_name\n",
    "    \n",
    "\n",
    "    def batch (self, dataframe: pd.DataFrame, col_name: str ='video_id', batch_size: int = 50) -> list[tuple[int,str]]:\n",
    "        \"\"\"YouTube Video API accepts up to 50 video IDs per requests. This function splits the full list of IDs into batches of 50 for multiple API calls.\"\"\"\n",
    "\n",
    "        if col_name not in dataframe.columns:\n",
    "            raise KeyError(f\"Column '{col_name}' not found in DataFrame\")\n",
    "        col = dataframe[col_name].astype(str)\n",
    "        col_length = len(col)\n",
    "        num_batch = (col_length // batch_size) + (1 if (col_length % batch_size) !=0 else 0)\n",
    "\n",
    "        return [\n",
    "        (i + 1, \",\".join(col.iloc[i*batch_size:(i+1)*batch_size].tolist()))\n",
    "        for i in range(num_batch)\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    def video(self, dataframe: pd.DataFrame):\n",
    "        \"\"\"Fetch video statistics for a set of IDs in batches and save to JSON files.\n",
    "\n",
    "        Uses `/youtube/v3/videos` with batched comma-separated IDs produced by `batch(...)`.\n",
    "        Each response is saved to `data/video/<batch_idx>.json`.\n",
    "\n",
    "        Args:\n",
    "            dataframe: Pandas DataFrame with a column named 'video_id' by default.\n",
    "\n",
    "        Returns:\n",
    "            Path to the folder containing saved JSON files (one per batch).\n",
    "\n",
    "        Notes:\n",
    "            - This method only requests `part=statistics`. Add `snippet,contentDetails`\n",
    "              if you need more fields.\n",
    "            - The `/videos` endpoint with `id=` param does NOT paginate with nextPageToken.\n",
    "              Each call returns stats for the provided list of IDs only.\n",
    "\n",
    "        Raises:\n",
    "            requests.RequestException: If a non-200/403 HTTP error occurs.\n",
    "        \"\"\"\n",
    "        ENDPOINT_AUTH = '/youtube/v3/videos'\n",
    "        folder_name = os.path.join(\"data\", f\"video\")\n",
    "        os.makedirs(folder_name, exist_ok=True)\n",
    "        response = {}\n",
    "        batches = self.batch(dataframe)\n",
    "\n",
    "        for batch_idx, batch in batches:\n",
    "            current_key = self.api_key\n",
    "            params = {\n",
    "                'part':'statistics',\n",
    "                'id':batch,\n",
    "                'key':current_key,\n",
    "                }\n",
    "\n",
    "            r = requests.get(\n",
    "                f\"{self.BASE_URL}{ENDPOINT_AUTH}\",\n",
    "                params=params,\n",
    "                headers=self.header,\n",
    "            )\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                print(f\"Code 200 - Using Quota\")\n",
    "                response = r.json()\n",
    "                filename= f\"{batch_idx}.json\"\n",
    "                file_path = os.path.join(folder_name, filename)\n",
    "\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                    json.dump(response, file, ensure_ascii=False, sort_keys=True, indent=2)\n",
    "\n",
    "                print(f\"Saved Json file page {filename}\")\n",
    "\n",
    "            elif r.status_code == 403:\n",
    "                print(\n",
    "                    f\"Key reached limit quota\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                print(f\"code {r.status_code}\")\n",
    "                break\n",
    "\n",
    "        return folder_name   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b8c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json (path: Path):\n",
    "    \"\"\"\n",
    "    Load a batch of YouTube API JSON files and flatten into a DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    all_data =[]\n",
    "\n",
    "    for p in path:\n",
    "        try:\n",
    "            with open(p, 'r', encoding='utf-8') as file:\n",
    "                record = json.load(file)\n",
    "        except json.JSONDecodeError as e: \n",
    "            print(f'Check {p}: {e}')\n",
    "        except Exception as e:\n",
    "            print(f'check {p}: {e}')\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if 'items' in record and isinstance(record['items'], list):\n",
    "                for item in record['items']:\n",
    "                    try:\n",
    "                        snippet = item['snippet']\n",
    "                        video_id = item['id']['videoId']\n",
    "                        \n",
    "                        flat_record = {\n",
    "                            'channel_id': snippet.get('channelId'),\n",
    "                            'channel_title': snippet.get('channelTitle'),\n",
    "                            'video_id': video_id,\n",
    "                            'title': snippet.get('title'),\n",
    "                            'publish_time': snippet.get('publishTime'),\n",
    "                        }\n",
    "                        all_data.append(flat_record)\n",
    "                    except:\n",
    "                        print (f'check {item}') \n",
    "            else:\n",
    "                print(f\"check file (no 'items'): {p}\")\n",
    "        except:\n",
    "            print(f'check {p}: {e}')\n",
    "\n",
    "    return pd.DataFrame(all_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
